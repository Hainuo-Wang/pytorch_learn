{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "pytorch深度学习实践\n",
    "\n",
    "https://www.bilibili.com/video/BV1Y7411d7Ys?p=1&vd_source=6d033c01bacc1b94de92d9ff542bdb52\n",
    "\n",
    "\n",
    "https://liuii.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "用PyTorch实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1、算预测值\n",
    "# 2、算loss\n",
    "# 3、梯度设为0，并反向传播\n",
    "# 3、梯度更新\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "# 构造线性模型,后面都是使用这样的模板\n",
    "# 至少实现两个函数，__init__构造函数和forward()前馈函数\n",
    "# backward()会根据我们的计算图自动构建\n",
    "# 可以继承Functions来构建自己的计算块\n",
    "class LinerModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # 调用父类的构造\n",
    "        super(LinerModel, self).__init__()\n",
    "        # 构造Linear这个对象，对输入数据做线性变换\n",
    "        # class torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        # in_features - 每个输入样本的大小\n",
    "        # out_features - 每个输出样本的大小\n",
    "        # bias - 若设置为False，这层不会学习偏置。默认值：True\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LinerModel()\n",
    "# 定义MSE(均方差)损失函数，size_average=False不求均值\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "# optim优化模块的SGD，第一个参数就是传递权重，model.parameters()model的所有权重\n",
    "# 优化器对象\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # loss为一个对象，但会自动调用__str__()所以不会出错\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # 梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 根据梯度和预先设置的学习率进行更新\n",
    "    optimizer.step()\n",
    "\n",
    "# 打印权重和偏置值,weight是一个值但是一个矩阵\n",
    "print('w=', model.linear.weight.item())\n",
    "print('b=', model.linear.bias.item())\n",
    "\n",
    "# 测试\n",
    "x_test = torch.Tensor([4.0])\n",
    "y_test = model(x_test)\n",
    "print('y_pred=', y_test.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "逻辑斯蒂回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 逻辑斯蒂回归\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[0], [0], [1]])\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将sigmoid函数应用到结果中\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LogisticRegressionModel()\n",
    "# 定义MSE(均方差)损失函数，size_average=False不求均值\n",
    "criterion = torch.nn.BCELoss(size_average=False)\n",
    "# optim优化模块的SGD，第一个参数就是传递权重，model.parameters()model的所有权重\n",
    "# 优化器对象\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # loss为一个对象，但会自动调用__str__()所以不会出错\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # 梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 根据梯度和预先蛇者的学习率进行更新\n",
    "    optimizer.step()\n",
    "\n",
    "# 打印权重和偏置值,weight是一个值但是一个矩阵\n",
    "print('w=', model.linear.weight.item())\n",
    "print('b=', model.linear.bias.item())\n",
    "\n",
    "# 测试\n",
    "x_test = torch.Tensor([4.0])\n",
    "y_test = model(x_test)\n",
    "print('y_pred=', y_test.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "处理多维特征的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1])\n",
    "# [-1]加中括号拿出来是矩阵，不加是向量\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        # 这是nn下的Sigmoid是一个模块没有参数，在function调用的Sigmoid是函数\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)  # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 优化函数，随机梯度递减\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 前馈\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    # 反馈\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset  # Dataset是一个抽象类，只能被继承，不能实例化\n",
    "from torch.utils.data import DataLoader  # 可以直接实例化\n",
    "\n",
    "'''\n",
    "四步：准备数据集-设计模型-构建损失函数和优化器-周期训练\n",
    "'''\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):  # 实例化对象后，该类能支持下标操作，通过index拿出数据\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset('diabetes.csv.gz')\n",
    "# dataset数据集，batch_size小批量的容量，shuffle是否要打乱，num_workers要几个并行进程来读\n",
    "# DataLoader的实例化对象不能直接使用，因为windows和linux的多线程运行不一样，所以一般要放在函数里运行\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        # 这是nn下的Sigmoid是一个模块没有参数，在function调用的Sigmoid是函数\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)  # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 优化函数，随机梯度递减\n",
    "\n",
    "# 变成嵌套循环，实现Mini-Batch\n",
    "for epoch in range(100):\n",
    "    # 从数据集0开始迭代\n",
    "    # 可以简写为for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 准备数据\n",
    "        inputs, labels = data\n",
    "        # 前馈\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.item())\n",
    "        # 反馈\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 更新\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784, 512)\n",
    "        self.l2 = torch.nn.Linear(512, 256)\n",
    "        self.l3 = torch.nn.Linear(256, 128)\n",
    "        self.l4 = torch.nn.Linear(128, 64)\n",
    "        self.l5 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 改变形状，相当于numpy的reshape\n",
    "        # view中一个参数定为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变。\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# model.parameters()直接使用的模型的所有参数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "卷积神经网络\n",
    "简单的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 输入的通道就是上图的n,输出的通道就是上图的m\n",
    "in_channels, out_channels = 5, 10\n",
    "width, height = 100, 100  # 图像的大小\n",
    "kernel_size = 3  # 卷积盒的大小\n",
    "batch_size = 1  # 批量大小\n",
    "\n",
    "# 随机生成了一个小批量=1的5*100*100的张量\n",
    "input = torch.randn(batch_size, in_channels, width, height)\n",
    "\n",
    "# Conv2d对由多个输入平面组成的输入信号进行二维卷积\n",
    "conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "output = conv_layer(input)\n",
    "\n",
    "# print(input)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "print(conv_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.Size([1, 5, 100, 100])\n",
    "torch.Size([1, 10, 98, 98])\n",
    "torch.Size([10, 5, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 4, 6, 5, 7,\n",
    "         2, 4, 6, 8, 2,\n",
    "         1, 6, 7, 8, 4,\n",
    "         9, 7, 4, 6, 2,\n",
    "         3, 7, 5, 4, 1]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 5, 5)\n",
    "\n",
    "# bias=False不加偏置量\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3)\n",
    "# 把kernel赋值给卷积层权重，做初始化\n",
    "conv_layer.weight.data = kernel.data\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[ 91., 168., 224., 215., 127.],\n",
    "          [114., 211., 295., 262., 149.],\n",
    "          [192., 259., 282., 214., 122.],\n",
    "          [194., 251., 253., 169.,  86.],\n",
    "          [ 96., 112., 110.,  68.,  31.]]]], grad_fn=<ThnnConv2DBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Layer-stride步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 4, 6, 5, 7,\n",
    "         2, 4, 6, 8, 2,\n",
    "         1, 6, 7, 8, 4,\n",
    "         9, 7, 4, 6, 2,\n",
    "         3, 7, 5, 4, 1]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 5, 5)\n",
    "\n",
    "# bias=False不加偏置量\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)\n",
    "\n",
    "kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3)\n",
    "# 把kernel赋值给卷积层权重，做初始化\n",
    "conv_layer.weight.data = kernel.data\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[211., 262.],\n",
    "          [251., 169.]]]], grad_fn=<ThnnConv2DBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Max Pooling Layer最大池化层（最大池化层是没有权重的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 9, 6, 5,\n",
    "         2, 4, 6, 8,\n",
    "         1, 6, 2, 1,\n",
    "         3, 7, 4, 6]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 4, 4)\n",
    "\n",
    "maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "output = maxpooling_layer(input)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[9., 8.],\n",
    "          [7., 6.]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 定义两个卷积层\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # 定义一个池化层\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        # 定义一个全连接的线性层\n",
    "        self.fc = torch.nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten data from (n, 1, 28, 28) to (n, 784)\n",
    "        # x.size(0)就是取的n\n",
    "        batch_size = x.size(0)\n",
    "        # 用relu做非线性激活\n",
    "        # 先做卷积再做池化再做relu\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        # 做view把数据变为做全连接网络所需要的输入\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n",
    "        # 因为最后一层要做交叉熵损失，所以最后一层不做激活\n",
    "\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "卷积神经网络（高级）GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        # 第一个通道，输入通道为in_channels,输出通道为16，卷积盒的大小为1*1的卷积层\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        # 第二个通道\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        # 第三个通道\n",
    "        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        # 第四个通道\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "        branch3x3 = self.branch3x3_3(branch3x3)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        # 拼接\n",
    "        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incep1 = InceptionA(in_channels=10)\n",
    "        self.incep2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incep1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Residual net残差结构块\n",
    "\n",
    "定义的该层输入和输出的大小是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.channels = channels\n",
    "        self.conv1 = nn.Conv2d(channels,channels,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels,channels,kernel_size=3,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "\n",
    "        self.rblock1 = ResidualBlock(16)\n",
    "        self.rblock2 = ResidualBlock(32)\n",
    "\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "        x = self.rblock2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1  # 批量数\n",
    "seq_len = 3  # 有几个输入队列x1,x2,x3\n",
    "input_size = 4  # 每个输入是几维向量\n",
    "hidden_size = 2  # 每个隐藏层是几维向量\n",
    "\n",
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "for idx, input in enumerate(dataset):\n",
    "    print('=' * 20, idx, '=' * 20)\n",
    "    print('Input size:', input.shape)\n",
    "\n",
    "    hidden = cell(input, hidden)\n",
    "\n",
    "    print('Outputs size:', hidden.shape)\n",
    "    print(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# (seqLen, batchSize, inputSize)\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "\n",
    "print('Output size:', out.shape)\n",
    "print('Output:', out)\n",
    "print('Hidden size: ', hidden.shape)\n",
    "print('Hidden: ', hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用RNNcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string: ehhhe, Epoch: [1/15] loss = 8.4598\n",
      "Predicted string: ehlhl, Epoch: [2/15] loss = 7.2599\n",
      "Predicted string: lhlel, Epoch: [3/15] loss = 6.3341\n",
      "Predicted string: lhlel, Epoch: [4/15] loss = 5.4571\n",
      "Predicted string: loleh, Epoch: [5/15] loss = 4.5418\n",
      "Predicted string: loleh, Epoch: [6/15] loss = 3.6996\n",
      "Predicted string: loleh, Epoch: [7/15] loss = 3.2128\n",
      "Predicted string: loleh, Epoch: [8/15] loss = 3.0056\n",
      "Predicted string: loleh, Epoch: [9/15] loss = 2.9081\n",
      "Predicted string: loleh, Epoch: [10/15] loss = 2.8354\n",
      "Predicted string: loleh, Epoch: [11/15] loss = 2.7565\n",
      "Predicted string: loleh, Epoch: [12/15] loss = 2.6594\n",
      "Predicted string: loleh, Epoch: [13/15] loss = 2.5422\n",
      "Predicted string: loleh, Epoch: [14/15] loss = 2.4102\n",
      "Predicted string: loleh, Epoch: [15/15] loss = 2.2700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]\n",
    "y_data = [2, 3, 2, 0, 1]\n",
    "\n",
    "# (seq_len, input_size, hidden_size)\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "inputs = torch.tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "inputs = inputs.float()\n",
    "\n",
    "y_one_hot = [one_hot_lookup[y] for y in y_data]\n",
    "labels = torch.tensor(y_one_hot).view(-1, batch_size, input_size)\n",
    "labels = labels.float()\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rnncell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnncell(input, hidden)\n",
    "        return hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "net = Net(input_size, hidden_size, batch_size)\n",
    "\n",
    "# construct Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "Optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = 0\n",
    "    net.zero_grad()\n",
    "    hidden = net.init_hidden()\n",
    "    print('Predicted string: ', end='')\n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden = net(input, hidden)\n",
    "        loss += criterion(hidden, label)\n",
    "        _, idx = hidden.max(dim=1)\n",
    "        print(idx2char[idx.item()], end='')\n",
    "\n",
    "    loss.backward()\n",
    "    Optimizer.step()\n",
    "    print(\", Epoch: [%d/15] loss = %.4f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  hhhhh, Epoch [1/15] loss = 1.204\n",
      "Predicted string:  hhllh, Epoch [2/15] loss = 1.105\n",
      "Predicted string:  lhlll, Epoch [3/15] loss = 1.026\n",
      "Predicted string:  lhlll, Epoch [4/15] loss = 0.968\n",
      "Predicted string:  ohlll, Epoch [5/15] loss = 0.918\n",
      "Predicted string:  ohlll, Epoch [6/15] loss = 0.873\n",
      "Predicted string:  ohlll, Epoch [7/15] loss = 0.829\n",
      "Predicted string:  ohlll, Epoch [8/15] loss = 0.788\n",
      "Predicted string:  ohlll, Epoch [9/15] loss = 0.750\n",
      "Predicted string:  oolll, Epoch [10/15] loss = 0.714\n",
      "Predicted string:  oolol, Epoch [11/15] loss = 0.683\n",
      "Predicted string:  oolol, Epoch [12/15] loss = 0.655\n",
      "Predicted string:  oolol, Epoch [13/15] loss = 0.626\n",
      "Predicted string:  oolol, Epoch [14/15] loss = 0.594\n",
      "Predicted string:  ohlol, Epoch [15/15] loss = 0.557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 3\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "seq_len = 5\n",
    "#构建输入输出字典\n",
    "idx2char_1 = ['e', 'h', 'l', 'o']\n",
    "idx2char_2 = ['h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]\n",
    "y_data = [2, 0, 1, 2, 1]\n",
    "# y_data = [3, 1, 2, 2, 3]\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)\n",
    "#labels（seqLen*batchSize,1）为了之后进行矩阵运算，计算交叉熵\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size #构造H0\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = torch.nn.RNN(input_size = self.input_size,\n",
    "                                hidden_size = self.hidden_size,\n",
    "                                num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(self.num_layers,\n",
    "                             self.batch_size,\n",
    "                             self.hidden_size)\n",
    "        out, _ = self.rnn(input, hidden)\n",
    "        #reshape成（SeqLen*batchsize,hiddensize）便于在进行交叉熵计算时可以以矩阵进行。\n",
    "        return out.view(-1, self.hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size, num_layers)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "#RNN中的输入（SeqLen*batchsize*inputsize）\n",
    "#RNN中的输出（SeqLen*batchsize*hiddensize）\n",
    "#labels维度 hiddensize*1\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char_2[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "embedding and linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "num_class = 4\n",
    "hidden_size = 8\n",
    "embedding_size =10\n",
    "batch_size = 1\n",
    "num_layers = 2\n",
    "seq_len = 5\n",
    "\n",
    "idx2char_1 = ['e', 'h', 'l', 'o']\n",
    "idx2char_2 = ['h', 'l', 'o']\n",
    "\n",
    "x_data = [[1, 0, 2, 2, 3]]\n",
    "y_data = [3, 1, 2, 2, 3]\n",
    "\n",
    "#inputs 作为交叉熵中的Inputs，维度为（batchsize，seqLen）\n",
    "inputs = torch.LongTensor(x_data)\n",
    "#labels 作为交叉熵中的Target，维度为（batchsize*seqLen）\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self .emb = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_size = embedding_size,\n",
    "                                hidden_size = hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first = True)\n",
    "                                \n",
    "        self.fc = torch.nn.Linear(hidden_size, num_class)\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, num_class)\n",
    "\n",
    "net = Model()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char_1[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "根据名字识别他所在的国家\n",
    "人名字符长短不一，最长的10个字符，所以处理成10维输入张量，都是英文字母刚好可以映射到ASCII上\n",
    "Maclean ->  ['M', 'a', 'c', 'l', 'e', 'a', 'n'] ->  [ 77 97 99 108 101 97 110]  ->  [ 77 97 99 108 101 97 110 0 0 0]\n",
    "共有18个国家，设置索引为0-17\n",
    "训练集和测试集的表格文件都是第一列人名，第二列国家\n",
    "'''\n",
    "import torch\n",
    "import  time\n",
    "import csv\n",
    "import gzip\n",
    "from  torch.utils.data import DataLoader\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128\n",
    "USE_GPU = True\n",
    "\n",
    "class NameDataset():         #处理数据集\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:    #打开压缩文件并将变量名设为为f\n",
    "            reader = csv.reader(f)              #读取表格文件\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]   #取出人名\n",
    "        self.len = len(self.names)              #人名数量\n",
    "        self.countries = [row[1] for row in rows]#取出国家名\n",
    "        self.country_list = list(sorted(set(self.countries)))#国家名集合，18个国家名的集合\n",
    "        #countrys是所有国家名，set(countrys)把所有国家明元素设为集合（去除重复项），sorted（）函数是将集合排序\n",
    "        #测试了一下，实际list(sorted(set(self.countrys)))==sorted(set(self.countrys))\n",
    "        self.country_dict = self.getCountryDict()#转变成词典\n",
    "        self.country_num = len(self.country_list)#得到国家集合的长度18\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    " \n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()                                       #创建空字典\n",
    "        for idx, country_name in enumerate(self.country_list,0):    #取出序号和对应国家名\n",
    "            country_dict[country_name] = idx                        #把对应的国家名和序号存入字典\n",
    "        return country_dict\n",
    " \n",
    "    def idx2country(self,index):            #返回索引对应国家名\n",
    "        return self.country_list(index)\n",
    " \n",
    "    def getCountrysNum(self):               #返回国家数量\n",
    "        return self.country_num\n",
    " \n",
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False)\n",
    " \n",
    "N_COUNTRY = trainset.getCountrysNum()       #模型输出大小\n",
    " \n",
    "def create_tensor(tensor):#判断是否使用GPU 使用的话把tensor搬到GPU上去\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    " \n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size                  #包括下面的n_layers在GRU模型里使用\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    " \n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)#input.shape=(seqlen,batch) output.shape=(seqlen,batch,hiddensize)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "                                #输入维度       输出维度      层数        说明单向还是双向\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)#双向GRU会输出两个hidden，维度需要✖2，要接一个线性层\n",
    " \n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()               #input shaoe :  Batch x Seq -> S x B 用于embedding\n",
    "        batch_size = input.size(1)\n",
    "        hidden =self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    " \n",
    "        # pack_padded_sequence函数当出入seq_lengths是GPU张量时报错，在这里改成cpu张量就可以，不用GPU直接注释掉下面这一行代码\n",
    "        seq_lengths = seq_lengths.cpu()#改成cpu张量\n",
    "        # pack them up\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths)#让0值不参与运算加快运算速度的方式\n",
    "        #需要提前把输入按有效值长度降序排列 再对输入做嵌入，然后按每个输入len（seq——lengths）取值做为GRU输入\n",
    " \n",
    "        output, hidden = self.gru(gru_input, hidden)#双向传播的话hidden有两个\n",
    "        if self.n_directions ==2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output\n",
    " \n",
    "    def _init_hidden(self,batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)\n",
    "        return  create_tensor(hidden)\n",
    " \n",
    "#classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    " \n",
    "#对名字的处理需要先把每个名字按字符都变成ASCII码\n",
    "def name2list(name):#把每个名字按字符都变成ASCII码\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    " \n",
    "def make_tensors(names, countries):     #处理名字ASCII码 重新排序的长度和国家列表\n",
    "    sequences_and_lengths= [name2list(name) for name in names]                  #把每个名字按字符都变成ASCII码\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths]                    #取出名字列表对应的ACSII码\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])     #取出每个名字对应的长度列表\n",
    "    countries = countries.long()\n",
    " \n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()     #先做一个 名字数量x最长名字长度的全0tensor\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):  #取出序列，ACSII码和长度列表\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)                       #用名字列表的ACSII码填充上面的全0tensor\n",
    " \n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)#将seq_lengths按序列长度重新降序排序，返回排序结果和排序序列。\n",
    "    seq_tensor = seq_tensor[perm_idx]                               #按新序列把ASCII表重新排序\n",
    "    countries = countries[perm_idx]                                 #按新序列把国家列表重新排序\n",
    " \n",
    "                #返回排序后的 ASCII列表         名字长度降序列表        国家名列表\n",
    "    return create_tensor(seq_tensor),create_tensor(seq_lengths),create_tensor(countries)\n",
    " \n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    " \n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)#取出排序后的 ASCII列表 名字长度列表 国家名列表\n",
    "        output = classifier(inputs, seq_lengths)    #把输入和序列放入分类器\n",
    "        loss = criterion(output, target)            #计算损失\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    " \n",
    "        #打印输出结果\n",
    "        #if i % 100 == 0:\n",
    "        #    print(f'Epoch {epoch} ')\n",
    "        if i == len(trainset) // BATCH_SIZE :\n",
    "            #print(f'[13374/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "        '''elif i % 10 == 9 :\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')'''\n",
    "    return total_loss\n",
    " \n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)    #返回处理后的名字ASCII码 重新排序的长度和国家列表\n",
    "            output = classifier(inputs, seq_lengths)                        #输出\n",
    "            pred = output.max(dim=1, keepdim=True)[1]                       #预测\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()           #计算预测对了多少\n",
    " \n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    print(\"Train for %d epochs...\" % N_EPOCHS)\n",
    "    start = time.time()\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        classifier.to(device)\n",
    " \n",
    "    criterion = torch.nn.CrossEntropyLoss()     #计算损失\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr = 0.001)   #更新\n",
    " \n",
    "    acc_list= []\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        #训练\n",
    "        print('%d / %d:' % (epoch, N_EPOCHS))\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc)\n",
    "    end = time.time()\n",
    "    print(datetime.timedelta(seconds=(end - start) // 1))\n",
    " \n",
    " \n",
    "    epoch = np.arange(1, len(acc_list) + 1, 1)\n",
    "    acc_list = np.array(acc_list)\n",
    "    plt.plot(epoch, acc_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "shelter_animals实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''在美国，每年大约有760万伴侣动物被动物收容所收容。大多数动物是被它们的主人主动放弃，而另一些则是由于种种的意外情况而进入收容所。最终，有些动物足够幸运找到了新的归宿，但另一些不那么幸运的则最终被安乐死。美国每年大约有２７０万的猫狗被执行安乐死。\n",
    "　　这次的比赛使用的是来自Austin的动物收容所的数据，其中包括动物的品种，颜色，性别和年龄，要求参赛者预测每只动物的最终结局。这些结局包括：被领养、死亡、安乐死、归还所有者和转移。其中训练集和测试集是随机划分的。\n",
    "　　最后输出测试集种每个动物的每一种结局的可能性即可。\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train = pd.read_csv('shelter_animals_train.csv')\n",
    "# print(\"Shape:\", train.shape)\n",
    "\n",
    "test = pd.read_csv('shelter_animals_test.csv')\n",
    "# print(\"Shape:\", test.shape)\n",
    "\n",
    "# Counter(train['OutcomeType'])\n",
    "# Counter(train['Name']).most_common(5)\n",
    "train_X = train.drop(columns=['OutcomeType', 'OutcomeSubtype', 'AnimalID'])\n",
    "Y = train['OutcomeType']\n",
    "test_X = test\n",
    "stacked_df = train_X.append(test_X.drop(columns=['ID']))\n",
    "stacked_df = stacked_df.drop(columns=['DateTime'])\n",
    "\n",
    "for col in stacked_df.columns:\n",
    "    if stacked_df[col].isnull().sum() > 10000:\n",
    "        # print(\"dropping\", col, stacked_df[col].isnull().sum())\n",
    "        stacked_df = stacked_df.drop(columns=[col])\n",
    "for col in stacked_df.columns:\n",
    "    if stacked_df.dtypes[col] == \"object\":\n",
    "        stacked_df[col] = stacked_df[col].fillna(\"NA\")\n",
    "    else:\n",
    "        stacked_df[col] = stacked_df[col].fillna(0)\n",
    "    stacked_df[col] = LabelEncoder().fit_transform(stacked_df[col])\n",
    "# making all variables categorical\n",
    "for col in stacked_df.columns:\n",
    "    stacked_df[col] = stacked_df[col].astype('category')\n",
    "X = stacked_df[0:26729]\n",
    "test_processed = stacked_df[26729:]\n",
    "# check if shape[0] matches original\n",
    "# print(\"train shape: \", X.shape, \"orignal: \", train.shape)\n",
    "# print(\"test shape: \", test_processed.shape, \"original: \", test.shape)\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "# sanity check to see numbers match and matching with previous counter to create target dictionary\n",
    "# print(Counter(train['OutcomeType']))\n",
    "# print(Counter(Y))\n",
    "target_dict = {\n",
    "    'Return_to_owner': 3,\n",
    "    'Euthanasia': 2,\n",
    "    'Adoption': 0,\n",
    "    'Transfer': 4,\n",
    "    'Died': 1\n",
    "}\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.10, random_state=0)\n",
    "\n",
    "# categorical embedding for columns having more than two values\n",
    "embedded_cols = {n: len(col.cat.categories) for n, col in X.items() if len(col.cat.categories) > 2}\n",
    "embedded_col_names = embedded_cols.keys()\n",
    "len(X.columns) - len(embedded_cols)  # number of numerical columns\n",
    "embedding_sizes = [(n_categories, min(50, (n_categories + 1) // 2)) for _, n_categories in embedded_cols.items()]\n",
    "\n",
    "print(\"X:\", type(X), X.shape)\n",
    "print(\"Y:\", type(Y), Y.shape)\n",
    "\n",
    "\n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:, embedded_col_names].copy().values.astype(np.int64)  # categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32)  # numerical columns\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):  # 返回单条数据\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# creating train and valid datasets\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "train_data_size = len(train_ds)\n",
    "valid_data_size = len(valid_ds)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "valid_ds_dataloader = DataLoader(valid_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)  # length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 5)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ShelterOutcomeModel(embedding_sizes, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = loss_func.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_dataloader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        x1, x2, y = data\n",
    "        # 把输入输出迁入GPU\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # forward+backward+update\n",
    "        outputs = model(x1, x2)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = loss_func(outputs, y.long())\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 5 == 0:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in valid_ds_dataloader:  # 拿数据\n",
    "            x1, x2, y = data\n",
    "            # 把输入输出迁入GPU\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            outputs = model(x1, x2)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += y.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == y).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    total_accuracy = []\n",
    "    for epoch in range(15):\n",
    "        train(epoch)\n",
    "        single_accuracy = test()\n",
    "        total_accuracy.append(single_accuracy)\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"ShelterAnimals\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.plot(range(15), total_accuracy)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f31b5846fe4d0e510ff280a80fa1fd1567c5c662c3b99a86eb737e0309da4a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}