{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "pytorch深度学习实践\n",
    "\n",
    "https://www.bilibili.com/video/BV1Y7411d7Ys?p=1&vd_source=6d033c01bacc1b94de92d9ff542bdb52\n",
    "\n",
    "\n",
    "https://liuii.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "用PyTorch实现线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1、算预测值\n",
    "# 2、算loss\n",
    "# 3、梯度设为0，并反向传播\n",
    "# 3、梯度更新\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "\n",
    "# 构造线性模型,后面都是使用这样的模板\n",
    "# 至少实现两个函数，__init__构造函数和forward()前馈函数\n",
    "# backward()会根据我们的计算图自动构建\n",
    "# 可以继承Functions来构建自己的计算块\n",
    "class LinerModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # 调用父类的构造\n",
    "        super(LinerModel, self).__init__()\n",
    "        # 构造Linear这个对象，对输入数据做线性变换\n",
    "        # class torch.nn.Linear(in_features, out_features, bias=True)\n",
    "        # in_features - 每个输入样本的大小\n",
    "        # out_features - 每个输出样本的大小\n",
    "        # bias - 若设置为False，这层不会学习偏置。默认值：True\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LinerModel()\n",
    "# 定义MSE(均方差)损失函数，size_average=False不求均值\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "# optim优化模块的SGD，第一个参数就是传递权重，model.parameters()model的所有权重\n",
    "# 优化器对象\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # loss为一个对象，但会自动调用__str__()所以不会出错\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # 梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 根据梯度和预先设置的学习率进行更新\n",
    "    optimizer.step()\n",
    "\n",
    "# 打印权重和偏置值,weight是一个值但是一个矩阵\n",
    "print('w=', model.linear.weight.item())\n",
    "print('b=', model.linear.bias.item())\n",
    "\n",
    "# 测试\n",
    "x_test = torch.Tensor([4.0])\n",
    "y_test = model(x_test)\n",
    "print('y_pred=', y_test.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "逻辑斯蒂回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 逻辑斯蒂回归\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.Tensor([[0], [0], [1]])\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将sigmoid函数应用到结果中\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = LogisticRegressionModel()\n",
    "# 定义MSE(均方差)损失函数，size_average=False不求均值\n",
    "criterion = torch.nn.BCELoss(size_average=False)\n",
    "# optim优化模块的SGD，第一个参数就是传递权重，model.parameters()model的所有权重\n",
    "# 优化器对象\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # loss为一个对象，但会自动调用__str__()所以不会出错\n",
    "    print(epoch, loss)\n",
    "\n",
    "    # 梯度归零\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    # 根据梯度和预先蛇者的学习率进行更新\n",
    "    optimizer.step()\n",
    "\n",
    "# 打印权重和偏置值,weight是一个值但是一个矩阵\n",
    "print('w=', model.linear.weight.item())\n",
    "print('b=', model.linear.bias.item())\n",
    "\n",
    "# 测试\n",
    "x_test = torch.Tensor([4.0])\n",
    "y_test = model(x_test)\n",
    "print('y_pred=', y_test.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "处理多维特征的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6563534736633301\n",
      "1 0.6555374264717102\n",
      "2 0.6547828912734985\n",
      "3 0.6540851593017578\n",
      "4 0.6534399390220642\n",
      "5 0.6528429388999939\n",
      "6 0.6522905826568604\n",
      "7 0.6517795324325562\n",
      "8 0.6513063311576843\n",
      "9 0.6508685350418091\n",
      "10 0.6504629850387573\n",
      "11 0.6500874757766724\n",
      "12 0.6497397422790527\n",
      "13 0.6494176387786865\n",
      "14 0.6491191983222961\n",
      "15 0.6488426923751831\n",
      "16 0.6485865712165833\n",
      "17 0.6483490467071533\n",
      "18 0.6481289267539978\n",
      "19 0.6479249000549316\n",
      "20 0.6477357149124146\n",
      "21 0.6475602388381958\n",
      "22 0.6473976373672485\n",
      "23 0.6472467184066772\n",
      "24 0.6471067667007446\n",
      "25 0.6469769477844238\n",
      "26 0.6468565464019775\n",
      "27 0.6467447876930237\n",
      "28 0.6466410756111145\n",
      "29 0.6465448141098022\n",
      "30 0.6464555263519287\n",
      "31 0.6463726758956909\n",
      "32 0.6462957859039307\n",
      "33 0.6462243795394897\n",
      "34 0.6461580395698547\n",
      "35 0.6460965275764465\n",
      "36 0.6460393071174622\n",
      "37 0.6459861993789673\n",
      "38 0.6459369659423828\n",
      "39 0.6458911299705505\n",
      "40 0.6458486914634705\n",
      "41 0.6458092331886292\n",
      "42 0.6457726359367371\n",
      "43 0.6457385420799255\n",
      "44 0.645706832408905\n",
      "45 0.6456774473190308\n",
      "46 0.6456501483917236\n",
      "47 0.6456246972084045\n",
      "48 0.6456011533737183\n",
      "49 0.6455792188644409\n",
      "50 0.6455588936805725\n",
      "51 0.6455399990081787\n",
      "52 0.6455223560333252\n",
      "53 0.6455060243606567\n",
      "54 0.645490825176239\n",
      "55 0.6454766988754272\n",
      "56 0.6454635858535767\n",
      "57 0.6454513072967529\n",
      "58 0.6454400420188904\n",
      "59 0.6454294919967651\n",
      "60 0.6454196572303772\n",
      "61 0.6454105377197266\n",
      "62 0.6454021334648132\n",
      "63 0.6453942060470581\n",
      "64 0.6453869342803955\n",
      "65 0.6453800797462463\n",
      "66 0.6453737020492554\n",
      "67 0.6453678607940674\n",
      "68 0.6453623175621033\n",
      "69 0.6453573107719421\n",
      "70 0.6453525424003601\n",
      "71 0.6453481316566467\n",
      "72 0.6453440189361572\n",
      "73 0.6453402042388916\n",
      "74 0.6453366875648499\n",
      "75 0.6453334093093872\n",
      "76 0.6453303098678589\n",
      "77 0.6453275084495544\n",
      "78 0.6453247666358948\n",
      "79 0.645322322845459\n",
      "80 0.645319938659668\n",
      "81 0.6453178524971008\n",
      "82 0.6453158855438232\n",
      "83 0.6453140377998352\n",
      "84 0.6453122496604919\n",
      "85 0.6453105807304382\n",
      "86 0.6453091502189636\n",
      "87 0.645307719707489\n",
      "88 0.6453064680099487\n",
      "89 0.6453052759170532\n",
      "90 0.6453041434288025\n",
      "91 0.6453030705451965\n",
      "92 0.6453020572662354\n",
      "93 0.6453012228012085\n",
      "94 0.6453003287315369\n",
      "95 0.6452995538711548\n",
      "96 0.6452987790107727\n",
      "97 0.6452980637550354\n",
      "98 0.6452974677085876\n",
      "99 0.6452968120574951\n",
      "100 0.6452962756156921\n",
      "101 0.6452957391738892\n",
      "102 0.645295262336731\n",
      "103 0.6452947854995728\n",
      "104 0.6452943682670593\n",
      "105 0.6452940106391907\n",
      "106 0.6452935934066772\n",
      "107 0.6452931761741638\n",
      "108 0.6452928781509399\n",
      "109 0.6452925205230713\n",
      "110 0.645292341709137\n",
      "111 0.6452920436859131\n",
      "112 0.645291805267334\n",
      "113 0.6452915668487549\n",
      "114 0.6452913284301758\n",
      "115 0.6452911496162415\n",
      "116 0.6452909708023071\n",
      "117 0.645290732383728\n",
      "118 0.6452906131744385\n",
      "119 0.6452904343605042\n",
      "120 0.645290195941925\n",
      "121 0.6452900767326355\n",
      "122 0.645289957523346\n",
      "123 0.6452897787094116\n",
      "124 0.6452896595001221\n",
      "125 0.6452895402908325\n",
      "126 0.6452895402908325\n",
      "127 0.645289421081543\n",
      "128 0.6452893018722534\n",
      "129 0.6452891826629639\n",
      "130 0.6452890634536743\n",
      "131 0.6452890634536743\n",
      "132 0.6452890038490295\n",
      "133 0.6452888250350952\n",
      "134 0.6452887654304504\n",
      "135 0.6452887058258057\n",
      "136 0.6452887058258057\n",
      "137 0.6452885866165161\n",
      "138 0.6452885270118713\n",
      "139 0.6452884674072266\n",
      "140 0.6452884674072266\n",
      "141 0.645288348197937\n",
      "142 0.6452882885932922\n",
      "143 0.6452881693840027\n",
      "144 0.6452881693840027\n",
      "145 0.6452881693840027\n",
      "146 0.6452881097793579\n",
      "147 0.6452880501747131\n",
      "148 0.6452879309654236\n",
      "149 0.6452879309654236\n",
      "150 0.6452878713607788\n",
      "151 0.6452878713607788\n",
      "152 0.645287811756134\n",
      "153 0.645287811756134\n",
      "154 0.6452876925468445\n",
      "155 0.6452876925468445\n",
      "156 0.6452876925468445\n",
      "157 0.6452876329421997\n",
      "158 0.6452875733375549\n",
      "159 0.6452875733375549\n",
      "160 0.6452874541282654\n",
      "161 0.6452874541282654\n",
      "162 0.6452873945236206\n",
      "163 0.6452874541282654\n",
      "164 0.6452873945236206\n",
      "165 0.6452873349189758\n",
      "166 0.6452873349189758\n",
      "167 0.645287275314331\n",
      "168 0.645287275314331\n",
      "169 0.645287275314331\n",
      "170 0.6452872157096863\n",
      "171 0.6452871561050415\n",
      "172 0.6452870965003967\n",
      "173 0.6452870965003967\n",
      "174 0.6452870965003967\n",
      "175 0.6452870965003967\n",
      "176 0.6452869772911072\n",
      "177 0.6452869772911072\n",
      "178 0.6452869772911072\n",
      "179 0.6452869176864624\n",
      "180 0.6452868580818176\n",
      "181 0.6452868580818176\n",
      "182 0.6452868580818176\n",
      "183 0.6452868580818176\n",
      "184 0.6452867388725281\n",
      "185 0.6452867388725281\n",
      "186 0.6452866792678833\n",
      "187 0.6452866196632385\n",
      "188 0.6452866196632385\n",
      "189 0.6452866196632385\n",
      "190 0.6452866196632385\n",
      "191 0.6452865600585938\n",
      "192 0.6452865600585938\n",
      "193 0.645286500453949\n",
      "194 0.6452864408493042\n",
      "195 0.6452863812446594\n",
      "196 0.6452863812446594\n",
      "197 0.6452863216400146\n",
      "198 0.6452863216400146\n",
      "199 0.6452863216400146\n",
      "200 0.6452862620353699\n",
      "201 0.6452862620353699\n",
      "202 0.6452862024307251\n",
      "203 0.6452862620353699\n",
      "204 0.6452862024307251\n",
      "205 0.6452862024307251\n",
      "206 0.6452860832214355\n",
      "207 0.6452860832214355\n",
      "208 0.6452860236167908\n",
      "209 0.6452860236167908\n",
      "210 0.645285964012146\n",
      "211 0.645285964012146\n",
      "212 0.645285964012146\n",
      "213 0.645285964012146\n",
      "214 0.6452859044075012\n",
      "215 0.6452858448028564\n",
      "216 0.6452858448028564\n",
      "217 0.6452858448028564\n",
      "218 0.6452858448028564\n",
      "219 0.6452857255935669\n",
      "220 0.6452857255935669\n",
      "221 0.6452857255935669\n",
      "222 0.6452856659889221\n",
      "223 0.6452856659889221\n",
      "224 0.6452856063842773\n",
      "225 0.6452855467796326\n",
      "226 0.6452856063842773\n",
      "227 0.6452855467796326\n",
      "228 0.6452854871749878\n",
      "229 0.6452854871749878\n",
      "230 0.6452854871749878\n",
      "231 0.645285427570343\n",
      "232 0.645285427570343\n",
      "233 0.6452853679656982\n",
      "234 0.6452853679656982\n",
      "235 0.6452853083610535\n",
      "236 0.6452853083610535\n",
      "237 0.6452852487564087\n",
      "238 0.6452852487564087\n",
      "239 0.6452852487564087\n",
      "240 0.6452851891517639\n",
      "241 0.6452851295471191\n",
      "242 0.6452851295471191\n",
      "243 0.6452850699424744\n",
      "244 0.6452850103378296\n",
      "245 0.6452850699424744\n",
      "246 0.6452850103378296\n",
      "247 0.6452850103378296\n",
      "248 0.64528489112854\n",
      "249 0.6452849507331848\n",
      "250 0.64528489112854\n",
      "251 0.64528489112854\n",
      "252 0.6452848315238953\n",
      "253 0.6452847719192505\n",
      "254 0.6452847719192505\n",
      "255 0.6452848315238953\n",
      "256 0.6452847123146057\n",
      "257 0.6452846527099609\n",
      "258 0.6452846527099609\n",
      "259 0.6452845931053162\n",
      "260 0.6452845931053162\n",
      "261 0.6452845931053162\n",
      "262 0.6452845931053162\n",
      "263 0.6452845335006714\n",
      "264 0.6452844738960266\n",
      "265 0.6452844738960266\n",
      "266 0.6452844738960266\n",
      "267 0.6452844738960266\n",
      "268 0.6452843546867371\n",
      "269 0.6452843546867371\n",
      "270 0.6452843546867371\n",
      "271 0.6452843546867371\n",
      "272 0.6452842950820923\n",
      "273 0.6452842354774475\n",
      "274 0.6452842354774475\n",
      "275 0.6452842354774475\n",
      "276 0.645284116268158\n",
      "277 0.645284116268158\n",
      "278 0.645284116268158\n",
      "279 0.645284116268158\n",
      "280 0.645284116268158\n",
      "281 0.6452840566635132\n",
      "282 0.6452839970588684\n",
      "283 0.6452839970588684\n",
      "284 0.6452839374542236\n",
      "285 0.6452839374542236\n",
      "286 0.6452838778495789\n",
      "287 0.6452838778495789\n",
      "288 0.6452838778495789\n",
      "289 0.6452838778495789\n",
      "290 0.6452838778495789\n",
      "291 0.6452837586402893\n",
      "292 0.6452837586402893\n",
      "293 0.6452837586402893\n",
      "294 0.6452837586402893\n",
      "295 0.6452836990356445\n",
      "296 0.6452836394309998\n",
      "297 0.6452836394309998\n",
      "298 0.6452836394309998\n",
      "299 0.6452835202217102\n",
      "300 0.645283579826355\n",
      "301 0.6452835202217102\n",
      "302 0.6452834606170654\n",
      "303 0.6452834606170654\n",
      "304 0.6452834606170654\n",
      "305 0.6452834010124207\n",
      "306 0.6452834010124207\n",
      "307 0.6452834010124207\n",
      "308 0.6452834010124207\n",
      "309 0.6452834010124207\n",
      "310 0.6452832818031311\n",
      "311 0.6452832221984863\n",
      "312 0.6452832221984863\n",
      "313 0.6452832221984863\n",
      "314 0.6452831625938416\n",
      "315 0.6452831029891968\n",
      "316 0.6452831029891968\n",
      "317 0.645283043384552\n",
      "318 0.645283043384552\n",
      "319 0.645283043384552\n",
      "320 0.6452829837799072\n",
      "321 0.6452829837799072\n",
      "322 0.6452829837799072\n",
      "323 0.6452829241752625\n",
      "324 0.6452828645706177\n",
      "325 0.6452828645706177\n",
      "326 0.6452828645706177\n",
      "327 0.6452828049659729\n",
      "328 0.6452828049659729\n",
      "329 0.6452827453613281\n",
      "330 0.6452827453613281\n",
      "331 0.6452827453613281\n",
      "332 0.6452827453613281\n",
      "333 0.6452827453613281\n",
      "334 0.6452826261520386\n",
      "335 0.6452826261520386\n",
      "336 0.6452826261520386\n",
      "337 0.6452826261520386\n",
      "338 0.645282506942749\n",
      "339 0.645282506942749\n",
      "340 0.645282506942749\n",
      "341 0.6452823877334595\n",
      "342 0.6452823877334595\n",
      "343 0.6452823877334595\n",
      "344 0.6452823877334595\n",
      "345 0.6452823281288147\n",
      "346 0.6452823281288147\n",
      "347 0.6452822685241699\n",
      "348 0.6452822685241699\n",
      "349 0.6452822685241699\n",
      "350 0.6452822685241699\n",
      "351 0.6452822089195251\n",
      "352 0.6452822089195251\n",
      "353 0.6452821493148804\n",
      "354 0.6452820301055908\n",
      "355 0.6452820301055908\n",
      "356 0.6452820301055908\n",
      "357 0.6452820301055908\n",
      "358 0.6452820301055908\n",
      "359 0.645281970500946\n",
      "360 0.645281970500946\n",
      "361 0.6452819108963013\n",
      "362 0.6452819108963013\n",
      "363 0.6452819108963013\n",
      "364 0.6452817916870117\n",
      "365 0.6452817916870117\n",
      "366 0.6452817916870117\n",
      "367 0.6452817916870117\n",
      "368 0.6452817916870117\n",
      "369 0.6452816724777222\n",
      "370 0.6452816724777222\n",
      "371 0.6452816724777222\n",
      "372 0.6452816724777222\n",
      "373 0.6452816724777222\n",
      "374 0.6452816724777222\n",
      "375 0.6452815532684326\n",
      "376 0.6452815532684326\n",
      "377 0.6452814340591431\n",
      "378 0.6452814340591431\n",
      "379 0.6452814340591431\n",
      "380 0.6452814340591431\n",
      "381 0.6452813744544983\n",
      "382 0.6452813744544983\n",
      "383 0.6452813148498535\n",
      "384 0.6452813148498535\n",
      "385 0.6452813148498535\n",
      "386 0.6452812552452087\n",
      "387 0.6452812552452087\n",
      "388 0.6452812552452087\n",
      "389 0.6452811360359192\n",
      "390 0.6452811360359192\n",
      "391 0.6452811360359192\n",
      "392 0.6452810764312744\n",
      "393 0.6452810764312744\n",
      "394 0.6452810764312744\n",
      "395 0.6452810764312744\n",
      "396 0.6452810168266296\n",
      "397 0.6452810168266296\n",
      "398 0.6452809572219849\n",
      "399 0.6452808976173401\n",
      "400 0.6452808976173401\n",
      "401 0.6452808976173401\n",
      "402 0.6452808976173401\n",
      "403 0.6452808380126953\n",
      "404 0.6452807784080505\n",
      "405 0.6452807784080505\n",
      "406 0.6452807784080505\n",
      "407 0.6452807188034058\n",
      "408 0.6452807188034058\n",
      "409 0.645280659198761\n",
      "410 0.645280659198761\n",
      "411 0.6452805995941162\n",
      "412 0.6452805995941162\n",
      "413 0.6452805995941162\n",
      "414 0.6452805399894714\n",
      "415 0.6452805399894714\n",
      "416 0.6452804803848267\n",
      "417 0.6452804803848267\n",
      "418 0.6452804803848267\n",
      "419 0.6452804207801819\n",
      "420 0.6452804207801819\n",
      "421 0.6452804207801819\n",
      "422 0.6452804207801819\n",
      "423 0.6452803015708923\n",
      "424 0.6452803015708923\n",
      "425 0.6452803015708923\n",
      "426 0.6452803015708923\n",
      "427 0.6452801823616028\n",
      "428 0.6452801823616028\n",
      "429 0.6452801823616028\n",
      "430 0.645280122756958\n",
      "431 0.645280122756958\n",
      "432 0.645280122756958\n",
      "433 0.6452800631523132\n",
      "434 0.645280122756958\n",
      "435 0.6452800631523132\n",
      "436 0.6452800035476685\n",
      "437 0.6452800035476685\n",
      "438 0.6452800035476685\n",
      "439 0.6452799439430237\n",
      "440 0.6452799439430237\n",
      "441 0.6452798843383789\n",
      "442 0.6452798247337341\n",
      "443 0.6452798247337341\n",
      "444 0.6452798247337341\n",
      "445 0.6452797651290894\n",
      "446 0.6452797651290894\n",
      "447 0.6452797055244446\n",
      "448 0.6452797055244446\n",
      "449 0.6452797055244446\n",
      "450 0.6452796459197998\n",
      "451 0.6452796459197998\n",
      "452 0.645279586315155\n",
      "453 0.6452796459197998\n",
      "454 0.6452795267105103\n",
      "455 0.6452795267105103\n",
      "456 0.6452795267105103\n",
      "457 0.6452794671058655\n",
      "458 0.6452794075012207\n",
      "459 0.6452794075012207\n",
      "460 0.6452794075012207\n",
      "461 0.6452794075012207\n",
      "462 0.6452794075012207\n",
      "463 0.6452794075012207\n",
      "464 0.6452793478965759\n",
      "465 0.6452793478965759\n",
      "466 0.6452792286872864\n",
      "467 0.6452791690826416\n",
      "468 0.6452791690826416\n",
      "469 0.6452791690826416\n",
      "470 0.6452791094779968\n",
      "471 0.6452791094779968\n",
      "472 0.6452791094779968\n",
      "473 0.6452791094779968\n",
      "474 0.645279049873352\n",
      "475 0.645279049873352\n",
      "476 0.645279049873352\n",
      "477 0.6452789902687073\n",
      "478 0.6452789306640625\n",
      "479 0.6452789306640625\n",
      "480 0.6452788710594177\n",
      "481 0.6452788710594177\n",
      "482 0.6452788710594177\n",
      "483 0.6452788710594177\n",
      "484 0.645278811454773\n",
      "485 0.645278811454773\n",
      "486 0.645278811454773\n",
      "487 0.6452786922454834\n",
      "488 0.6452786922454834\n",
      "489 0.6452786922454834\n",
      "490 0.6452786922454834\n",
      "491 0.6452786922454834\n",
      "492 0.6452785730361938\n",
      "493 0.6452785730361938\n",
      "494 0.6452785730361938\n",
      "495 0.6452785730361938\n",
      "496 0.6452784538269043\n",
      "497 0.6452784538269043\n",
      "498 0.6452784538269043\n",
      "499 0.6452784538269043\n",
      "500 0.6452783942222595\n",
      "501 0.6452783942222595\n",
      "502 0.6452783942222595\n",
      "503 0.6452783346176147\n",
      "504 0.64527827501297\n",
      "505 0.6452783346176147\n",
      "506 0.6452782154083252\n",
      "507 0.6452782154083252\n",
      "508 0.6452782154083252\n",
      "509 0.6452782154083252\n",
      "510 0.6452782154083252\n",
      "511 0.6452781558036804\n",
      "512 0.6452781558036804\n",
      "513 0.6452780961990356\n",
      "514 0.6452780961990356\n",
      "515 0.6452780365943909\n",
      "516 0.6452780365943909\n",
      "517 0.6452780365943909\n",
      "518 0.6452779173851013\n",
      "519 0.6452779173851013\n",
      "520 0.6452779173851013\n",
      "521 0.6452779173851013\n",
      "522 0.6452779173851013\n",
      "523 0.6452779173851013\n",
      "524 0.6452777981758118\n",
      "525 0.6452777981758118\n",
      "526 0.6452777981758118\n",
      "527 0.645277738571167\n",
      "528 0.645277738571167\n",
      "529 0.6452776789665222\n",
      "530 0.6452776789665222\n",
      "531 0.6452776193618774\n",
      "532 0.6452776193618774\n",
      "533 0.6452776193618774\n",
      "534 0.6452775597572327\n",
      "535 0.6452775597572327\n",
      "536 0.6452775597572327\n",
      "537 0.6452775001525879\n",
      "538 0.6452774405479431\n",
      "539 0.6452774405479431\n",
      "540 0.6452773809432983\n",
      "541 0.6452773809432983\n",
      "542 0.6452773809432983\n",
      "543 0.6452773213386536\n",
      "544 0.6452773213386536\n",
      "545 0.6452773213386536\n",
      "546 0.6452773213386536\n",
      "547 0.6452772617340088\n",
      "548 0.6452772617340088\n",
      "549 0.645277202129364\n",
      "550 0.645277202129364\n",
      "551 0.645277202129364\n",
      "552 0.645277202129364\n",
      "553 0.6452770829200745\n",
      "554 0.6452770829200745\n",
      "555 0.6452770829200745\n",
      "556 0.6452770829200745\n",
      "557 0.6452770829200745\n",
      "558 0.6452770233154297\n",
      "559 0.6452769637107849\n",
      "560 0.6452769637107849\n",
      "561 0.6452769041061401\n",
      "562 0.6452769041061401\n",
      "563 0.6452769041061401\n",
      "564 0.6452768445014954\n",
      "565 0.6452768445014954\n",
      "566 0.6452768445014954\n",
      "567 0.6452767848968506\n",
      "568 0.6452767848968506\n",
      "569 0.6452767252922058\n",
      "570 0.645276665687561\n",
      "571 0.645276665687561\n",
      "572 0.645276665687561\n",
      "573 0.645276665687561\n",
      "574 0.645276665687561\n",
      "575 0.6452766060829163\n",
      "576 0.6452766060829163\n",
      "577 0.6452766060829163\n",
      "578 0.6452766060829163\n",
      "579 0.6452764868736267\n",
      "580 0.6452764868736267\n",
      "581 0.6452764868736267\n",
      "582 0.6452764272689819\n",
      "583 0.6452763080596924\n",
      "584 0.6452764272689819\n",
      "585 0.6452763080596924\n",
      "586 0.6452763080596924\n",
      "587 0.6452763080596924\n",
      "588 0.6452763080596924\n",
      "589 0.6452763080596924\n",
      "590 0.6452763080596924\n",
      "591 0.6452761888504028\n",
      "592 0.6452761888504028\n",
      "593 0.6452761888504028\n",
      "594 0.6452761292457581\n",
      "595 0.6452760696411133\n",
      "596 0.6452760696411133\n",
      "597 0.6452760696411133\n",
      "598 0.6452760100364685\n",
      "599 0.6452760100364685\n",
      "600 0.6452760100364685\n",
      "601 0.6452759504318237\n",
      "602 0.645275890827179\n",
      "603 0.645275890827179\n",
      "604 0.645275890827179\n",
      "605 0.645275890827179\n",
      "606 0.645275890827179\n",
      "607 0.645275890827179\n",
      "608 0.6452758312225342\n",
      "609 0.6452758312225342\n",
      "610 0.6452757120132446\n",
      "611 0.6452757120132446\n",
      "612 0.6452757120132446\n",
      "613 0.6452757120132446\n",
      "614 0.6452757120132446\n",
      "615 0.6452755928039551\n",
      "616 0.6452755928039551\n",
      "617 0.6452755928039551\n",
      "618 0.6452755928039551\n",
      "619 0.6452755928039551\n",
      "620 0.6452754735946655\n",
      "621 0.6452754735946655\n",
      "622 0.6452754735946655\n",
      "623 0.6452754735946655\n",
      "624 0.6452754139900208\n",
      "625 0.645275354385376\n",
      "626 0.645275354385376\n",
      "627 0.645275354385376\n",
      "628 0.645275354385376\n",
      "629 0.645275354385376\n",
      "630 0.6452752947807312\n",
      "631 0.6452752947807312\n",
      "632 0.6452752351760864\n",
      "633 0.6452752351760864\n",
      "634 0.6452751159667969\n",
      "635 0.6452751159667969\n",
      "636 0.6452751159667969\n",
      "637 0.6452751159667969\n",
      "638 0.6452751159667969\n",
      "639 0.6452750563621521\n",
      "640 0.6452750563621521\n",
      "641 0.6452750563621521\n",
      "642 0.6452749967575073\n",
      "643 0.6452749967575073\n",
      "644 0.6452749967575073\n",
      "645 0.6452749371528625\n",
      "646 0.6452749371528625\n",
      "647 0.6452749371528625\n",
      "648 0.6452748775482178\n",
      "649 0.6452748775482178\n",
      "650 0.6452748775482178\n",
      "651 0.645274817943573\n",
      "652 0.6452747583389282\n",
      "653 0.6452747583389282\n",
      "654 0.6452747583389282\n",
      "655 0.6452747583389282\n",
      "656 0.6452747583389282\n",
      "657 0.6452746391296387\n",
      "658 0.6452746391296387\n",
      "659 0.6452745795249939\n",
      "660 0.6452745795249939\n",
      "661 0.6452745795249939\n",
      "662 0.6452745795249939\n",
      "663 0.6452745199203491\n",
      "664 0.6452744603157043\n",
      "665 0.6452744603157043\n",
      "666 0.6452744603157043\n",
      "667 0.6452744007110596\n",
      "668 0.6452744007110596\n",
      "669 0.6452744007110596\n",
      "670 0.6452743411064148\n",
      "671 0.6452743411064148\n",
      "672 0.6452743411064148\n",
      "673 0.64527428150177\n",
      "674 0.64527428150177\n",
      "675 0.6452742218971252\n",
      "676 0.6452742218971252\n",
      "677 0.6452742218971252\n",
      "678 0.6452741622924805\n",
      "679 0.6452741026878357\n",
      "680 0.6452741026878357\n",
      "681 0.6452741026878357\n",
      "682 0.6452741026878357\n",
      "683 0.6452741026878357\n",
      "684 0.6452740430831909\n",
      "685 0.6452740430831909\n",
      "686 0.6452739834785461\n",
      "687 0.6452739834785461\n",
      "688 0.6452739834785461\n",
      "689 0.6452738642692566\n",
      "690 0.6452738642692566\n",
      "691 0.6452738642692566\n",
      "692 0.6452738046646118\n",
      "693 0.6452738046646118\n",
      "694 0.6452738046646118\n",
      "695 0.6452738046646118\n",
      "696 0.645273745059967\n",
      "697 0.645273745059967\n",
      "698 0.6452736854553223\n",
      "699 0.6452736854553223\n",
      "700 0.6452736854553223\n",
      "701 0.6452736258506775\n",
      "702 0.6452736258506775\n",
      "703 0.6452736258506775\n",
      "704 0.6452736258506775\n",
      "705 0.6452735662460327\n",
      "706 0.6452735662460327\n",
      "707 0.6452735662460327\n",
      "708 0.6452735066413879\n",
      "709 0.6452733874320984\n",
      "710 0.6452733874320984\n",
      "711 0.6452733874320984\n",
      "712 0.6452733874320984\n",
      "713 0.6452733874320984\n",
      "714 0.6452733278274536\n",
      "715 0.6452733278274536\n",
      "716 0.6452733278274536\n",
      "717 0.6452732682228088\n",
      "718 0.6452732682228088\n",
      "719 0.6452732682228088\n",
      "720 0.6452732682228088\n",
      "721 0.6452731490135193\n",
      "722 0.6452731490135193\n",
      "723 0.6452731490135193\n",
      "724 0.6452731490135193\n",
      "725 0.6452731490135193\n",
      "726 0.6452730894088745\n",
      "727 0.6452730894088745\n",
      "728 0.6452730894088745\n",
      "729 0.6452730298042297\n",
      "730 0.6452730298042297\n",
      "731 0.6452730298042297\n",
      "732 0.645272970199585\n",
      "733 0.6452729105949402\n",
      "734 0.6452728509902954\n",
      "735 0.6452728509902954\n",
      "736 0.6452727913856506\n",
      "737 0.6452727913856506\n",
      "738 0.6452728509902954\n",
      "739 0.6452727317810059\n",
      "740 0.6452727317810059\n",
      "741 0.6452727317810059\n",
      "742 0.6452727317810059\n",
      "743 0.6452726721763611\n",
      "744 0.6452726125717163\n",
      "745 0.6452726125717163\n",
      "746 0.6452726125717163\n",
      "747 0.6452726125717163\n",
      "748 0.6452726125717163\n",
      "749 0.6452725529670715\n",
      "750 0.6452725529670715\n",
      "751 0.6452724933624268\n",
      "752 0.6452724933624268\n",
      "753 0.6452724933624268\n",
      "754 0.6452724933624268\n",
      "755 0.645272433757782\n",
      "756 0.6452723741531372\n",
      "757 0.6452723741531372\n",
      "758 0.6452723741531372\n",
      "759 0.6452723741531372\n",
      "760 0.6452723741531372\n",
      "761 0.6452723145484924\n",
      "762 0.6452722549438477\n",
      "763 0.6452722549438477\n",
      "764 0.6452721953392029\n",
      "765 0.6452721953392029\n",
      "766 0.6452721953392029\n",
      "767 0.6452721953392029\n",
      "768 0.6452721357345581\n",
      "769 0.6452720761299133\n",
      "770 0.6452721357345581\n",
      "771 0.6452720165252686\n",
      "772 0.6452720165252686\n",
      "773 0.6452720165252686\n",
      "774 0.6452720165252686\n",
      "775 0.6452720165252686\n",
      "776 0.645271897315979\n",
      "777 0.645271897315979\n",
      "778 0.645271897315979\n",
      "779 0.645271897315979\n",
      "780 0.645271897315979\n",
      "781 0.645271897315979\n",
      "782 0.6452718377113342\n",
      "783 0.6452718377113342\n",
      "784 0.6452718377113342\n",
      "785 0.6452718377113342\n",
      "786 0.6452717185020447\n",
      "787 0.6452717185020447\n",
      "788 0.6452717185020447\n",
      "789 0.6452716588973999\n",
      "790 0.6452716588973999\n",
      "791 0.6452715992927551\n",
      "792 0.6452715992927551\n",
      "793 0.6452715992927551\n",
      "794 0.6452715992927551\n",
      "795 0.6452715396881104\n",
      "796 0.6452714800834656\n",
      "797 0.6452714800834656\n",
      "798 0.6452714800834656\n",
      "799 0.6452714800834656\n",
      "800 0.6452714204788208\n",
      "801 0.6452714204788208\n",
      "802 0.645271360874176\n",
      "803 0.645271360874176\n",
      "804 0.645271360874176\n",
      "805 0.645271360874176\n",
      "806 0.645271360874176\n",
      "807 0.6452712416648865\n",
      "808 0.6452713012695312\n",
      "809 0.6452712416648865\n",
      "810 0.6452711224555969\n",
      "811 0.6452711224555969\n",
      "812 0.6452711224555969\n",
      "813 0.6452711224555969\n",
      "814 0.6452711224555969\n",
      "815 0.6452710628509521\n",
      "816 0.6452710628509521\n",
      "817 0.6452710032463074\n",
      "818 0.6452710032463074\n",
      "819 0.6452710032463074\n",
      "820 0.6452710032463074\n",
      "821 0.6452709436416626\n",
      "822 0.6452709436416626\n",
      "823 0.6452709436416626\n",
      "824 0.6452708840370178\n",
      "825 0.6452708840370178\n",
      "826 0.645270824432373\n",
      "827 0.6452708840370178\n",
      "828 0.6452708840370178\n",
      "829 0.6452707648277283\n",
      "830 0.6452707648277283\n",
      "831 0.6452707648277283\n",
      "832 0.6452706456184387\n",
      "833 0.6452706456184387\n",
      "834 0.6452706456184387\n",
      "835 0.6452706456184387\n",
      "836 0.6452706456184387\n",
      "837 0.6452706456184387\n",
      "838 0.6452706456184387\n",
      "839 0.645270586013794\n",
      "840 0.645270586013794\n",
      "841 0.6452705264091492\n",
      "842 0.6452705264091492\n",
      "843 0.6452704071998596\n",
      "844 0.6452704071998596\n",
      "845 0.6452704071998596\n",
      "846 0.6452704071998596\n",
      "847 0.6452704071998596\n",
      "848 0.6452703475952148\n",
      "849 0.6452703475952148\n",
      "850 0.6452703475952148\n",
      "851 0.6452702879905701\n",
      "852 0.6452702879905701\n",
      "853 0.6452702879905701\n",
      "854 0.6452702879905701\n",
      "855 0.6452701687812805\n",
      "856 0.6452701687812805\n",
      "857 0.6452701687812805\n",
      "858 0.6452701687812805\n",
      "859 0.6452701687812805\n",
      "860 0.6452701091766357\n",
      "861 0.645270049571991\n",
      "862 0.645270049571991\n",
      "863 0.645270049571991\n",
      "864 0.645270049571991\n",
      "865 0.645270049571991\n",
      "866 0.645270049571991\n",
      "867 0.6452699303627014\n",
      "868 0.6452699303627014\n",
      "869 0.6452699303627014\n",
      "870 0.6452698707580566\n",
      "871 0.6452698707580566\n",
      "872 0.6452698707580566\n",
      "873 0.6452698707580566\n",
      "874 0.6452698111534119\n",
      "875 0.6452698111534119\n",
      "876 0.6452698111534119\n",
      "877 0.6452697515487671\n",
      "878 0.6452696919441223\n",
      "879 0.6452696919441223\n",
      "880 0.6452696919441223\n",
      "881 0.6452696919441223\n",
      "882 0.6452696323394775\n",
      "883 0.6452696919441223\n",
      "884 0.6452696323394775\n",
      "885 0.6452695727348328\n",
      "886 0.6452695727348328\n",
      "887 0.645269513130188\n",
      "888 0.6452694535255432\n",
      "889 0.6452694535255432\n",
      "890 0.6452694535255432\n",
      "891 0.6452694535255432\n",
      "892 0.6452693939208984\n",
      "893 0.6452693939208984\n",
      "894 0.6452693939208984\n",
      "895 0.6452693939208984\n",
      "896 0.6452693343162537\n",
      "897 0.6452693343162537\n",
      "898 0.6452692747116089\n",
      "899 0.6452692747116089\n",
      "900 0.6452692151069641\n",
      "901 0.6452692151069641\n",
      "902 0.6452691555023193\n",
      "903 0.6452691555023193\n",
      "904 0.6452690958976746\n",
      "905 0.6452691555023193\n",
      "906 0.6452690958976746\n",
      "907 0.6452690958976746\n",
      "908 0.6452690958976746\n",
      "909 0.6452690958976746\n",
      "910 0.6452690362930298\n",
      "911 0.6452690362930298\n",
      "912 0.645268976688385\n",
      "913 0.6452689170837402\n",
      "914 0.6452689170837402\n",
      "915 0.6452689170837402\n",
      "916 0.6452688574790955\n",
      "917 0.6452689170837402\n",
      "918 0.6452688574790955\n",
      "919 0.6452688574790955\n",
      "920 0.6452688574790955\n",
      "921 0.6452688574790955\n",
      "922 0.6452687382698059\n",
      "923 0.6452687382698059\n",
      "924 0.6452686786651611\n",
      "925 0.6452686786651611\n",
      "926 0.6452686786651611\n",
      "927 0.6452686786651611\n",
      "928 0.6452685594558716\n",
      "929 0.6452686190605164\n",
      "930 0.6452685594558716\n",
      "931 0.6452685594558716\n",
      "932 0.6452685594558716\n",
      "933 0.6452684998512268\n",
      "934 0.6452684998512268\n",
      "935 0.6452684998512268\n",
      "936 0.645268440246582\n",
      "937 0.645268440246582\n",
      "938 0.645268440246582\n",
      "939 0.645268440246582\n",
      "940 0.645268440246582\n",
      "941 0.6452683806419373\n",
      "942 0.6452683806419373\n",
      "943 0.6452683210372925\n",
      "944 0.6452683806419373\n",
      "945 0.6452682614326477\n",
      "946 0.6452682614326477\n",
      "947 0.6452682018280029\n",
      "948 0.6452682018280029\n",
      "949 0.6452682018280029\n",
      "950 0.6452681422233582\n",
      "951 0.6452681422233582\n",
      "952 0.6452681422233582\n",
      "953 0.6452681422233582\n",
      "954 0.6452680826187134\n",
      "955 0.6452680826187134\n",
      "956 0.6452680826187134\n",
      "957 0.6452680826187134\n",
      "958 0.6452680230140686\n",
      "959 0.6452679634094238\n",
      "960 0.6452679634094238\n",
      "961 0.6452679634094238\n",
      "962 0.645267903804779\n",
      "963 0.645267903804779\n",
      "964 0.6452678442001343\n",
      "965 0.6452678442001343\n",
      "966 0.6452677845954895\n",
      "967 0.6452677845954895\n",
      "968 0.6452677845954895\n",
      "969 0.6452677845954895\n",
      "970 0.6452677845954895\n",
      "971 0.6452677845954895\n",
      "972 0.6452677845954895\n",
      "973 0.6452676653862\n",
      "974 0.6452676653862\n",
      "975 0.6452676057815552\n",
      "976 0.6452676057815552\n",
      "977 0.6452676057815552\n",
      "978 0.6452676057815552\n",
      "979 0.6452675461769104\n",
      "980 0.6452675461769104\n",
      "981 0.6452675461769104\n",
      "982 0.6452675461769104\n",
      "983 0.6452674269676208\n",
      "984 0.6452674269676208\n",
      "985 0.6452674269676208\n",
      "986 0.6452674269676208\n",
      "987 0.6452673673629761\n",
      "988 0.6452673673629761\n",
      "989 0.6452673673629761\n",
      "990 0.6452673077583313\n",
      "991 0.6452673077583313\n",
      "992 0.6452673077583313\n",
      "993 0.6452673077583313\n",
      "994 0.6452671885490417\n",
      "995 0.6452671885490417\n",
      "996 0.6452671885490417\n",
      "997 0.6452671885490417\n",
      "998 0.6452671885490417\n",
      "999 0.645267128944397\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype=np.float32)\n",
    "x_data = torch.from_numpy(xy[:, :-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "# mean, std = torch.mean(x_data), torch.std(x_data)\n",
    "# x_data = (x_data-mean)/std\n",
    "# mean, std = torch.mean(y_data), torch.std(y_data)\n",
    "# y_data = (y_data-mean)/std\n",
    "# x_data = torch.nn.functional.normalize(x_data, dim=0)\n",
    "# y_data = torch.nn.functional.normalize(y_data, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 4)\n",
    "        self.linear2 = torch.nn.Linear(4, 2)\n",
    "        self.linear3 = torch.nn.Linear(2, 1)\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)  # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 优化函数，随机梯度递减\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 前馈\n",
    "    y_pred = model(x_data)\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.item())\n",
    "\n",
    "    # 反馈\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset  # Dataset是一个抽象类，只能被继承，不能实例化\n",
    "from torch.utils.data import DataLoader  # 可以直接实例化\n",
    "\n",
    "'''\n",
    "四步：准备数据集-设计模型-构建损失函数和优化器-周期训练\n",
    "'''\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, :-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):  # 实例化对象后，该类能支持下标操作，通过index拿出数据\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset('diabetes.csv.gz')\n",
    "# dataset数据集，batch_size小批量的容量，shuffle是否要打乱，num_workers要几个并行进程来读\n",
    "# DataLoader的实例化对象不能直接使用，因为windows和linux的多线程运行不一样，所以一般要放在函数里运行\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 6)\n",
    "        self.linear2 = torch.nn.Linear(6, 4)\n",
    "        self.linear3 = torch.nn.Linear(4, 1)\n",
    "        # 这是nn下的Sigmoid是一个模块没有参数，在function调用的Sigmoid是函数\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.linear1(x))\n",
    "        x = self.sigmoid(self.linear2(x))\n",
    "        x = self.sigmoid(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = torch.nn.BCELoss(size_average=True)  # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 优化函数，随机梯度递减\n",
    "\n",
    "# 变成嵌套循环，实现Mini-Batch\n",
    "for epoch in range(100):\n",
    "    # 从数据集0开始迭代\n",
    "    # 可以简写为for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # 准备数据\n",
    "        inputs, labels = data\n",
    "        # 前馈\n",
    "        y_pred = model(inputs)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.item())\n",
    "        # 反馈\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 更新\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(784, 512)\n",
    "        self.l2 = torch.nn.Linear(512, 256)\n",
    "        self.l3 = torch.nn.Linear(256, 128)\n",
    "        self.l4 = torch.nn.Linear(128, 64)\n",
    "        self.l5 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 改变形状，相当于numpy的reshape\n",
    "        # view中一个参数定为-1，代表动态调整这个维度上的元素个数，以保证元素的总数不变。\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# model.parameters()直接使用的模型的所有参数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "卷积神经网络\n",
    "简单的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 输入的通道就是上图的n,输出的通道就是上图的m\n",
    "in_channels, out_channels = 5, 10\n",
    "width, height = 100, 100  # 图像的大小\n",
    "kernel_size = 3  # 卷积盒的大小\n",
    "batch_size = 1  # 批量大小\n",
    "\n",
    "# 随机生成了一个小批量=1的5*100*100的张量\n",
    "input = torch.randn(batch_size, in_channels, width, height)\n",
    "\n",
    "# Conv2d对由多个输入平面组成的输入信号进行二维卷积\n",
    "conv_layer = torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "output = conv_layer(input)\n",
    "\n",
    "# print(input)\n",
    "print(input.shape)\n",
    "print(output.shape)\n",
    "print(conv_layer.weight.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.Size([1, 5, 100, 100])\n",
    "torch.Size([1, 10, 98, 98])\n",
    "torch.Size([10, 5, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 4, 6, 5, 7,\n",
    "         2, 4, 6, 8, 2,\n",
    "         1, 6, 7, 8, 4,\n",
    "         9, 7, 4, 6, 2,\n",
    "         3, 7, 5, 4, 1]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 5, 5)\n",
    "\n",
    "# bias=False不加偏置量\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3)\n",
    "# 把kernel赋值给卷积层权重，做初始化\n",
    "conv_layer.weight.data = kernel.data\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[ 91., 168., 224., 215., 127.],\n",
    "          [114., 211., 295., 262., 149.],\n",
    "          [192., 259., 282., 214., 122.],\n",
    "          [194., 251., 253., 169.,  86.],\n",
    "          [ 96., 112., 110.,  68.,  31.]]]], grad_fn=<ThnnConv2DBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Layer-stride步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 4, 6, 5, 7,\n",
    "         2, 4, 6, 8, 2,\n",
    "         1, 6, 7, 8, 4,\n",
    "         9, 7, 4, 6, 2,\n",
    "         3, 7, 5, 4, 1]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 5, 5)\n",
    "\n",
    "# bias=False不加偏置量\n",
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)\n",
    "\n",
    "kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(1, 1, 3, 3)\n",
    "# 把kernel赋值给卷积层权重，做初始化\n",
    "conv_layer.weight.data = kernel.data\n",
    "\n",
    "output = conv_layer(input)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[211., 262.],\n",
    "          [251., 169.]]]], grad_fn=<ThnnConv2DBackward>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Max Pooling Layer最大池化层（最大池化层是没有权重的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = [3, 9, 6, 5,\n",
    "         2, 4, 6, 8,\n",
    "         1, 6, 2, 1,\n",
    "         3, 7, 4, 6]\n",
    "\n",
    "input = torch.Tensor(input).view(1, 1, 4, 4)\n",
    "\n",
    "maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "output = maxpooling_layer(input)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor([[[[9., 8.],\n",
    "          [7., 6.]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 定义两个卷积层\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # 定义一个池化层\n",
    "        self.pooling = torch.nn.MaxPool2d(2)\n",
    "        # 定义一个全连接的线性层\n",
    "        self.fc = torch.nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten data from (n, 1, 28, 28) to (n, 784)\n",
    "        # x.size(0)就是取的n\n",
    "        batch_size = x.size(0)\n",
    "        # 用relu做非线性激活\n",
    "        # 先做卷积再做池化再做relu\n",
    "        x = F.relu(self.pooling(self.conv1(x)))\n",
    "        x = F.relu(self.pooling(self.conv2(x)))\n",
    "        # 做view把数据变为做全连接网络所需要的输入\n",
    "        x = x.view(batch_size, -1)\n",
    "        return self.fc(x)\n",
    "        # 因为最后一层要做交叉熵损失，所以最后一层不做激活\n",
    "\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "卷积神经网络（高级）GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class InceptionA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        # 第一个通道，输入通道为in_channels,输出通道为16，卷积盒的大小为1*1的卷积层\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        # 第二个通道\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        # 第三个通道\n",
    "        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        # 第四个通道\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "        branch3x3 = self.branch3x3_3(branch3x3)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        # 拼接\n",
    "        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
    "\n",
    "        self.incep1 = InceptionA(in_channels=10)\n",
    "        self.incep2 = InceptionA(in_channels=20)\n",
    "\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incep1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incep2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Residual net残差结构块\n",
    "\n",
    "定义的该层输入和输出的大小是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms  # 对图像进行处理的工具\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F  # 使用激活函数relu()的包\n",
    "import torch.optim as optim  # 优化器的包\n",
    "\n",
    "batch_size = 64\n",
    "# 对图像进行预处理，将图像转换为\n",
    "transform = transforms.Compose([\n",
    "    # 将原始图像PIL变为张量tensor(H*W*C),再将[0,255]区间转换为[0.1,1.0]\n",
    "    transforms.ToTensor(),\n",
    "    # 使用均值和标准差对张量图像进行归一化\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/mnist/', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.channels = channels\n",
    "        self.conv1 = nn.Conv2d(channels,channels,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels,channels,kernel_size=3,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x+y)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "\n",
    "        self.rblock1 = ResidualBlock(16)\n",
    "        self.rblock2 = ResidualBlock(32)\n",
    "\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = self.mp(F.relu(self.conv1(x)))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.mp(F.relu(self.conv2(x)))\n",
    "        x = self.rblock2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = Net()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 交叉熵损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)  # momentum动量\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        inputs, target = data\n",
    "        # 把输入输出迁入GPU\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward+backward+update\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = criterion(outputs, target)\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 300 == 299:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # 拿数据\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += labels.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1  # 批量数\n",
    "seq_len = 3  # 有几个输入队列x1,x2,x3\n",
    "input_size = 4  # 每个输入是几维向量\n",
    "hidden_size = 2  # 每个隐藏层是几维向量\n",
    "\n",
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "for idx, input in enumerate(dataset):\n",
    "    print('=' * 20, idx, '=' * 20)\n",
    "    print('Input size:', input.shape)\n",
    "\n",
    "    hidden = cell(input, hidden)\n",
    "\n",
    "    print('Outputs size:', hidden.shape)\n",
    "    print(hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# (seqLen, batchSize, inputSize)\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "\n",
    "print('Output size:', out.shape)\n",
    "print('Output:', out)\n",
    "print('Hidden size: ', hidden.shape)\n",
    "print('Hidden: ', hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用RNNcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string: ehhhe, Epoch: [1/15] loss = 8.4598\n",
      "Predicted string: ehlhl, Epoch: [2/15] loss = 7.2599\n",
      "Predicted string: lhlel, Epoch: [3/15] loss = 6.3341\n",
      "Predicted string: lhlel, Epoch: [4/15] loss = 5.4571\n",
      "Predicted string: loleh, Epoch: [5/15] loss = 4.5418\n",
      "Predicted string: loleh, Epoch: [6/15] loss = 3.6996\n",
      "Predicted string: loleh, Epoch: [7/15] loss = 3.2128\n",
      "Predicted string: loleh, Epoch: [8/15] loss = 3.0056\n",
      "Predicted string: loleh, Epoch: [9/15] loss = 2.9081\n",
      "Predicted string: loleh, Epoch: [10/15] loss = 2.8354\n",
      "Predicted string: loleh, Epoch: [11/15] loss = 2.7565\n",
      "Predicted string: loleh, Epoch: [12/15] loss = 2.6594\n",
      "Predicted string: loleh, Epoch: [13/15] loss = 2.5422\n",
      "Predicted string: loleh, Epoch: [14/15] loss = 2.4102\n",
      "Predicted string: loleh, Epoch: [15/15] loss = 2.2700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]\n",
    "y_data = [2, 3, 2, 0, 1]\n",
    "\n",
    "# (seq_len, input_size, hidden_size)\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "inputs = torch.tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "inputs = inputs.float()\n",
    "\n",
    "y_one_hot = [one_hot_lookup[y] for y in y_data]\n",
    "labels = torch.tensor(y_one_hot).view(-1, batch_size, input_size)\n",
    "labels = labels.float()\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.rnncell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = self.rnncell(input, hidden)\n",
    "        return hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "net = Net(input_size, hidden_size, batch_size)\n",
    "\n",
    "# construct Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "Optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = 0\n",
    "    net.zero_grad()\n",
    "    hidden = net.init_hidden()\n",
    "    print('Predicted string: ', end='')\n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden = net(input, hidden)\n",
    "        loss += criterion(hidden, label)\n",
    "        _, idx = hidden.max(dim=1)\n",
    "        print(idx2char[idx.item()], end='')\n",
    "\n",
    "    loss.backward()\n",
    "    Optimizer.step()\n",
    "    print(\", Epoch: [%d/15] loss = %.4f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  hhhhh, Epoch [1/15] loss = 1.204\n",
      "Predicted string:  hhllh, Epoch [2/15] loss = 1.105\n",
      "Predicted string:  lhlll, Epoch [3/15] loss = 1.026\n",
      "Predicted string:  lhlll, Epoch [4/15] loss = 0.968\n",
      "Predicted string:  ohlll, Epoch [5/15] loss = 0.918\n",
      "Predicted string:  ohlll, Epoch [6/15] loss = 0.873\n",
      "Predicted string:  ohlll, Epoch [7/15] loss = 0.829\n",
      "Predicted string:  ohlll, Epoch [8/15] loss = 0.788\n",
      "Predicted string:  ohlll, Epoch [9/15] loss = 0.750\n",
      "Predicted string:  oolll, Epoch [10/15] loss = 0.714\n",
      "Predicted string:  oolol, Epoch [11/15] loss = 0.683\n",
      "Predicted string:  oolol, Epoch [12/15] loss = 0.655\n",
      "Predicted string:  oolol, Epoch [13/15] loss = 0.626\n",
      "Predicted string:  oolol, Epoch [14/15] loss = 0.594\n",
      "Predicted string:  ohlol, Epoch [15/15] loss = 0.557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 3\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "seq_len = 5\n",
    "#构建输入输出字典\n",
    "idx2char_1 = ['e', 'h', 'l', 'o']\n",
    "idx2char_2 = ['h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 2, 3]\n",
    "y_data = [2, 0, 1, 2, 1]\n",
    "# y_data = [3, 1, 2, 2, 3]\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)\n",
    "#labels（seqLen*batchSize,1）为了之后进行矩阵运算，计算交叉熵\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size #构造H0\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = torch.nn.RNN(input_size = self.input_size,\n",
    "                                hidden_size = self.hidden_size,\n",
    "                                num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(self.num_layers,\n",
    "                             self.batch_size,\n",
    "                             self.hidden_size)\n",
    "        out, _ = self.rnn(input, hidden)\n",
    "        #reshape成（SeqLen*batchsize,hiddensize）便于在进行交叉熵计算时可以以矩阵进行。\n",
    "        return out.view(-1, self.hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size, num_layers)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "#RNN中的输入（SeqLen*batchsize*inputsize）\n",
    "#RNN中的输出（SeqLen*batchsize*hiddensize）\n",
    "#labels维度 hiddensize*1\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char_2[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "embedding and linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "num_class = 4\n",
    "hidden_size = 8\n",
    "embedding_size =10\n",
    "batch_size = 1\n",
    "num_layers = 2\n",
    "seq_len = 5\n",
    "\n",
    "idx2char_1 = ['e', 'h', 'l', 'o']\n",
    "idx2char_2 = ['h', 'l', 'o']\n",
    "\n",
    "x_data = [[1, 0, 2, 2, 3]]\n",
    "y_data = [3, 1, 2, 2, 3]\n",
    "\n",
    "#inputs 作为交叉熵中的Inputs，维度为（batchsize，seqLen）\n",
    "inputs = torch.LongTensor(x_data)\n",
    "#labels 作为交叉熵中的Target，维度为（batchsize*seqLen）\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self .emb = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_size = embedding_size,\n",
    "                                hidden_size = hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first = True)\n",
    "                                \n",
    "        self.fc = torch.nn.Linear(hidden_size, num_class)\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, num_class)\n",
    "\n",
    "net = Model()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char_1[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "根据名字识别他所在的国家\n",
    "人名字符长短不一，最长的10个字符，所以处理成10维输入张量，都是英文字母刚好可以映射到ASCII上\n",
    "Maclean ->  ['M', 'a', 'c', 'l', 'e', 'a', 'n'] ->  [ 77 97 99 108 101 97 110]  ->  [ 77 97 99 108 101 97 110 0 0 0]\n",
    "共有18个国家，设置索引为0-17\n",
    "训练集和测试集的表格文件都是第一列人名，第二列国家\n",
    "'''\n",
    "import torch\n",
    "import  time\n",
    "import csv\n",
    "import gzip\n",
    "from  torch.utils.data import DataLoader\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128\n",
    "USE_GPU = True\n",
    "\n",
    "class NameDataset():         #处理数据集\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:    #打开压缩文件并将变量名设为为f\n",
    "            reader = csv.reader(f)              #读取表格文件\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]   #取出人名\n",
    "        self.len = len(self.names)              #人名数量\n",
    "        self.countries = [row[1] for row in rows]#取出国家名\n",
    "        self.country_list = list(sorted(set(self.countries)))#国家名集合，18个国家名的集合\n",
    "        #countrys是所有国家名，set(countrys)把所有国家明元素设为集合（去除重复项），sorted（）函数是将集合排序\n",
    "        #测试了一下，实际list(sorted(set(self.countrys)))==sorted(set(self.countrys))\n",
    "        self.country_dict = self.getCountryDict()#转变成词典\n",
    "        self.country_num = len(self.country_list)#得到国家集合的长度18\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    " \n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()                                       #创建空字典\n",
    "        for idx, country_name in enumerate(self.country_list,0):    #取出序号和对应国家名\n",
    "            country_dict[country_name] = idx                        #把对应的国家名和序号存入字典\n",
    "        return country_dict\n",
    " \n",
    "    def idx2country(self,index):            #返回索引对应国家名\n",
    "        return self.country_list(index)\n",
    " \n",
    "    def getCountrysNum(self):               #返回国家数量\n",
    "        return self.country_num\n",
    " \n",
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False)\n",
    " \n",
    "N_COUNTRY = trainset.getCountrysNum()       #模型输出大小\n",
    " \n",
    "def create_tensor(tensor):#判断是否使用GPU 使用的话把tensor搬到GPU上去\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        tensor = tensor.to(device)\n",
    "    return tensor\n",
    " \n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size                  #包括下面的n_layers在GRU模型里使用\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1\n",
    " \n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)#input.shape=(seqlen,batch) output.shape=(seqlen,batch,hiddensize)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "                                #输入维度       输出维度      层数        说明单向还是双向\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)#双向GRU会输出两个hidden，维度需要✖2，要接一个线性层\n",
    " \n",
    "    def forward(self, input, seq_lengths):\n",
    "        input = input.t()               #input shaoe :  Batch x Seq -> S x B 用于embedding\n",
    "        batch_size = input.size(1)\n",
    "        hidden =self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    " \n",
    "        # pack_padded_sequence函数当出入seq_lengths是GPU张量时报错，在这里改成cpu张量就可以，不用GPU直接注释掉下面这一行代码\n",
    "        seq_lengths = seq_lengths.cpu()#改成cpu张量\n",
    "        # pack them up\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths)#让0值不参与运算加快运算速度的方式\n",
    "        #需要提前把输入按有效值长度降序排列 再对输入做嵌入，然后按每个输入len（seq——lengths）取值做为GRU输入\n",
    " \n",
    "        output, hidden = self.gru(gru_input, hidden)#双向传播的话hidden有两个\n",
    "        if self.n_directions ==2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output\n",
    " \n",
    "    def _init_hidden(self,batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)\n",
    "        return  create_tensor(hidden)\n",
    " \n",
    "#classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    " \n",
    "#对名字的处理需要先把每个名字按字符都变成ASCII码\n",
    "def name2list(name):#把每个名字按字符都变成ASCII码\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    " \n",
    "def make_tensors(names, countries):     #处理名字ASCII码 重新排序的长度和国家列表\n",
    "    sequences_and_lengths= [name2list(name) for name in names]                  #把每个名字按字符都变成ASCII码\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths]                    #取出名字列表对应的ACSII码\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])     #取出每个名字对应的长度列表\n",
    "    countries = countries.long()\n",
    " \n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()     #先做一个 名字数量x最长名字长度的全0tensor\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):  #取出序列，ACSII码和长度列表\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)                       #用名字列表的ACSII码填充上面的全0tensor\n",
    " \n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)#将seq_lengths按序列长度重新降序排序，返回排序结果和排序序列。\n",
    "    seq_tensor = seq_tensor[perm_idx]                               #按新序列把ASCII表重新排序\n",
    "    countries = countries[perm_idx]                                 #按新序列把国家列表重新排序\n",
    " \n",
    "                #返回排序后的 ASCII列表         名字长度降序列表        国家名列表\n",
    "    return create_tensor(seq_tensor),create_tensor(seq_lengths),create_tensor(countries)\n",
    " \n",
    "def trainModel():\n",
    "    total_loss = 0\n",
    " \n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)#取出排序后的 ASCII列表 名字长度列表 国家名列表\n",
    "        output = classifier(inputs, seq_lengths)    #把输入和序列放入分类器\n",
    "        loss = criterion(output, target)            #计算损失\n",
    " \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    " \n",
    "        #打印输出结果\n",
    "        #if i % 100 == 0:\n",
    "        #    print(f'Epoch {epoch} ')\n",
    "        if i == len(trainset) // BATCH_SIZE :\n",
    "            #print(f'[13374/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "        '''elif i % 10 == 9 :\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')'''\n",
    "    return total_loss\n",
    " \n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)    #返回处理后的名字ASCII码 重新排序的长度和国家列表\n",
    "            output = classifier(inputs, seq_lengths)                        #输出\n",
    "            pred = output.max(dim=1, keepdim=True)[1]                       #预测\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()           #计算预测对了多少\n",
    " \n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    print(\"Train for %d epochs...\" % N_EPOCHS)\n",
    "    start = time.time()\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    if USE_GPU:\n",
    "        device = torch.device('cuda:0')\n",
    "        classifier.to(device)\n",
    " \n",
    "    criterion = torch.nn.CrossEntropyLoss()     #计算损失\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr = 0.001)   #更新\n",
    " \n",
    "    acc_list= []\n",
    "    for epoch in range(1, N_EPOCHS+1):\n",
    "        #训练\n",
    "        print('%d / %d:' % (epoch, N_EPOCHS))\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        acc_list.append(acc)\n",
    "    end = time.time()\n",
    "    print(datetime.timedelta(seconds=(end - start) // 1))\n",
    " \n",
    " \n",
    "    epoch = np.arange(1, len(acc_list) + 1, 1)\n",
    "    acc_list = np.array(acc_list)\n",
    "    plt.plot(epoch, acc_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "shelter_animals实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''在美国，每年大约有760万伴侣动物被动物收容所收容。大多数动物是被它们的主人主动放弃，而另一些则是由于种种的意外情况而进入收容所。最终，有些动物足够幸运找到了新的归宿，但另一些不那么幸运的则最终被安乐死。美国每年大约有２７０万的猫狗被执行安乐死。\n",
    "　　这次的比赛使用的是来自Austin的动物收容所的数据，其中包括动物的品种，颜色，性别和年龄，要求参赛者预测每只动物的最终结局。这些结局包括：被领养、死亡、安乐死、归还所有者和转移。其中训练集和测试集是随机划分的。\n",
    "　　最后输出测试集种每个动物的每一种结局的可能性即可。\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train = pd.read_csv('shelter_animals_train.csv')\n",
    "# print(\"Shape:\", train.shape)\n",
    "\n",
    "test = pd.read_csv('shelter_animals_test.csv')\n",
    "# print(\"Shape:\", test.shape)\n",
    "\n",
    "# Counter(train['OutcomeType'])\n",
    "# Counter(train['Name']).most_common(5)\n",
    "train_X = train.drop(columns=['OutcomeType', 'OutcomeSubtype', 'AnimalID'])\n",
    "Y = train['OutcomeType']\n",
    "test_X = test\n",
    "stacked_df = train_X.append(test_X.drop(columns=['ID']))\n",
    "stacked_df = stacked_df.drop(columns=['DateTime'])\n",
    "\n",
    "for col in stacked_df.columns:\n",
    "    if stacked_df[col].isnull().sum() > 10000:\n",
    "        # print(\"dropping\", col, stacked_df[col].isnull().sum())\n",
    "        stacked_df = stacked_df.drop(columns=[col])\n",
    "for col in stacked_df.columns:\n",
    "    if stacked_df.dtypes[col] == \"object\":\n",
    "        stacked_df[col] = stacked_df[col].fillna(\"NA\")\n",
    "    else:\n",
    "        stacked_df[col] = stacked_df[col].fillna(0)\n",
    "    stacked_df[col] = LabelEncoder().fit_transform(stacked_df[col])\n",
    "# making all variables categorical\n",
    "for col in stacked_df.columns:\n",
    "    stacked_df[col] = stacked_df[col].astype('category')\n",
    "X = stacked_df[0:26729]\n",
    "test_processed = stacked_df[26729:]\n",
    "# check if shape[0] matches original\n",
    "# print(\"train shape: \", X.shape, \"orignal: \", train.shape)\n",
    "# print(\"test shape: \", test_processed.shape, \"original: \", test.shape)\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "# sanity check to see numbers match and matching with previous counter to create target dictionary\n",
    "# print(Counter(train['OutcomeType']))\n",
    "# print(Counter(Y))\n",
    "target_dict = {\n",
    "    'Return_to_owner': 3,\n",
    "    'Euthanasia': 2,\n",
    "    'Adoption': 0,\n",
    "    'Transfer': 4,\n",
    "    'Died': 1\n",
    "}\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.10, random_state=0)\n",
    "\n",
    "# categorical embedding for columns having more than two values\n",
    "embedded_cols = {n: len(col.cat.categories) for n, col in X.items() if len(col.cat.categories) > 2}\n",
    "embedded_col_names = embedded_cols.keys()\n",
    "len(X.columns) - len(embedded_cols)  # number of numerical columns\n",
    "embedding_sizes = [(n_categories, min(50, (n_categories + 1) // 2)) for _, n_categories in embedded_cols.items()]\n",
    "\n",
    "print(\"X:\", type(X), X.shape)\n",
    "print(\"Y:\", type(Y), Y.shape)\n",
    "\n",
    "\n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:, embedded_col_names].copy().values.astype(np.int64)  # categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32)  # numerical columns\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):  # 返回单条数据\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "\n",
    "\n",
    "# creating train and valid datasets\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "train_data_size = len(train_ds)\n",
    "valid_data_size = len(valid_ds)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "valid_ds_dataloader = DataLoader(valid_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories, size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings)  # length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 5)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ShelterOutcomeModel(embedding_sizes, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = loss_func.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "    # 返回了数据下标和数据\n",
    "    for batch_idx, data in enumerate(train_dataloader, 0):\n",
    "        # 送入两个张量，一个张量是64个图像的特征，一个张量图片对应的数字\n",
    "        x1, x2, y = data\n",
    "        # 把输入输出迁入GPU\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # forward+backward+update\n",
    "        outputs = model(x1, x2)\n",
    "        # 计算损失，用的交叉熵损失函数\n",
    "        loss = loss_func(outputs, y.long())\n",
    "        # 反馈\n",
    "        loss.backward()\n",
    "        # 随机梯度下降更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 每300次输出一次\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 5 == 0:\n",
    "            print('[%d,%5d] loss:%.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 不会计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in valid_ds_dataloader:  # 拿数据\n",
    "            x1, x2, y = data\n",
    "            # 把输入输出迁入GPU\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            outputs = model(x1, x2)  # 预测\n",
    "            # outputs.data是一个矩阵，每一行10个量，最大值的下标就是预测值\n",
    "            _, predicted = torch.max(outputs.data, dim=1)  # 沿着第一维度，找最大值的下标，返回最大值和下标\n",
    "            total += y.size(0)  # labels.size(0)=64 每个都是64个元素，就可以计算总的元素\n",
    "            # (predicted == labels).sum()这个是张量，而加了item()变为一个数字，即相等的数量\n",
    "            correct += (predicted == y).sum().item()\n",
    "    print('Accuracy on test set:%d %%' % (100 * correct / total))  # 正确的数量除以总数\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    total_accuracy = []\n",
    "    for epoch in range(15):\n",
    "        train(epoch)\n",
    "        single_accuracy = test()\n",
    "        total_accuracy.append(single_accuracy)\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"ShelterAnimals\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.plot(range(15), total_accuracy)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f31b5846fe4d0e510ff280a80fa1fd1567c5c662c3b99a86eb737e0309da4a2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}